{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/18 19:50:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/18 19:50:50 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/03/18 19:50:50 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_id                                   application_1727681258360_0139\n"
     ]
    }
   ],
   "source": [
    "conf = (SparkConf().setAppName('Transform_Method').setMaster('yarn') \n",
    "    .set('spark.sql.adaptive.enabled', False)  )\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print('app_id'.ljust(40), spark.sparkContext.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e783ae2e-440a-4b25-b444-acbf52bbabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import when, dense_rank, avg, col, desc, lit\n",
    "\n",
    "def fair_avg(df, col_gr, col_val, n):\n",
    "    col_gr = df[col_gr] if isinstance(col_gr, str) else col_gr\n",
    "    col_val = df[col_val] if isinstance(col_val, str) else col_val\n",
    "    col_val_name = col_val._jc.toString()\n",
    "\n",
    "    wind_asc = Window.partitionBy(col_gr).orderBy(col_val)\n",
    "    wind_desc = Window.partitionBy(col_gr).orderBy(col_val.desc())\n",
    "    df = df\\\n",
    "        .withColumn('rnk_asc', dense_rank().over(wind_asc))\\\n",
    "        .withColumn('rnk_dsc', dense_rank().over(wind_desc))\\\n",
    "        .withColumn(col_val_name, \\\n",
    "                    when((col('rnk_asc')>n) & (col('rnk_dsc')>n), col_val)\\\n",
    "                         .otherwise(lit(None)))\\\n",
    "        .drop(col('rnk_asc'), col('rnk_dsc'))\n",
    "\n",
    "    return df.groupBy(col_gr).agg(\n",
    "                               avg(col_val_name).alias('avg_'+col_val_name)\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ec51b7-906e-4a7f-8c64-e88a4851afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('client1', 20), ('client1', 10), ('client1', 40), \n",
    "        ('client1', 20), ('client1', 50), ('client1', 40),\n",
    "        ('client1', 50), ('client1', 60), ('client1', 30),\n",
    "\n",
    "        ('client2', 20), ('client2', 50), ('client2', 40), \n",
    "        ('client2', 40), ('client2', 10), ('client2', 30),\n",
    "        ('client2', 50), ('client2', 60), ('client2', 10)]\n",
    "\n",
    "df_data = spark.createDataFrame(data, 'id: string, number: long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd98453-1bfc-4da3-8df1-04e3849383a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|     id|       avg_number|\n",
      "+-------+-----------------+\n",
      "|client2|34.44444444444444|\n",
      "|client1|35.55555555555556|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|        avg_number|\n",
      "+-------+------------------+\n",
      "|client2|38.333333333333336|\n",
      "|client1|35.714285714285715|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|        avg_number|\n",
      "+-------+------------------+\n",
      "|client2|36.666666666666664|\n",
      "|client1|36.666666666666664|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_data.transform(fair_avg, 'id', 'number', 0).show()\n",
    "df_data.transform(fair_avg, 'id', 'number', 1).show()\n",
    "df_data.transform(fair_avg, 'id', 'number', 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b97768-54eb-4116-b165-80d27bd2bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vat(df_sales, country_code_col, val_col, pc=0.15):\n",
    "    #... \n",
    "    return df_sales\n",
    "\n",
    "def with_cashback(df_sales, mcc_code):\n",
    "    #... \n",
    "    return df_sales\n",
    "    \n",
    "def in_promo(df_sales, cmpgn_id):\n",
    "    #... \n",
    "    return df_sales\n",
    "    \n",
    "df_res = df_data.transform(exclude_vat, 'country_code', 'amt')\\\n",
    "                 .transform(with_cashback, '4509')\\\n",
    "                 .transform(in_promo, 2047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
