{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('first_last_window_funcs').setMaster('yarn')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8127ef-cc7f-4467-b992-02571b857b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "   select order_id, order_date \n",
    "    from values \n",
    "     (1, '2025-09-08'), (2, '2025-09-12'), \n",
    "     (5, '2025-09-14'), (6, '2025-09-24'), \n",
    "     (3, '2025-10-04'), (4, '2025-10-09'), \n",
    "     (7, '2025-10-22'), (8, '2025-10-28')\n",
    "   as (order_id, order_date) \"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('tbl_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d780e1-707c-4fa8-b148-1dc3e1866667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|order_date|order_mon|date_first_mon_order|\n",
      "+----------+---------+--------------------+\n",
      "|2025-09-08|  2025-09|          2025-09-08|\n",
      "|2025-09-12|  2025-09|          2025-09-08|\n",
      "|2025-09-14|  2025-09|          2025-09-08|\n",
      "|2025-09-24|  2025-09|          2025-09-08|\n",
      "|2025-10-04|  2025-10|          2025-10-04|\n",
      "|2025-10-09|  2025-10|          2025-10-04|\n",
      "|2025-10-22|  2025-10|          2025-10-04|\n",
      "|2025-10-28|  2025-10|          2025-10-04|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"  \n",
    "  with cte as (\n",
    "  select \n",
    "    order_date, \n",
    "    substr(order_date, 1, 7) as order_mon\n",
    "   from tbl_orders \n",
    "  )\n",
    "  select \n",
    "    order_date, order_mon,\n",
    "    first_value(order_date) over(partition by order_mon order by order_date)\n",
    "      as date_first_mon_order\n",
    "   from cte\n",
    "  order by order_date \"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('tbl_orders_2')\n",
    "spark.table(\"tbl_orders_2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a32606-aa7a-4d9f-87b1-6a9f5a4b3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+---------+\n",
      "|order_date|order_mon|date_first_mon_order|days_diff|\n",
      "+----------+---------+--------------------+---------+\n",
      "|2025-09-08|  2025-09|          2025-09-08|        0|\n",
      "|2025-09-12|  2025-09|          2025-09-08|        4|\n",
      "|2025-09-14|  2025-09|          2025-09-08|        6|\n",
      "|2025-09-24|  2025-09|          2025-09-08|       16|\n",
      "|2025-10-04|  2025-10|          2025-10-04|        0|\n",
      "|2025-10-09|  2025-10|          2025-10-04|        5|\n",
      "|2025-10-22|  2025-10|          2025-10-04|       18|\n",
      "|2025-10-28|  2025-10|          2025-10-04|       24|\n",
      "+----------+---------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"  \n",
    "  select \n",
    "    order_date, order_mon, date_first_mon_order,\n",
    "    datediff(order_date, date_first_mon_order) days_diff\n",
    "   from tbl_orders_2\n",
    "  order by order_date \"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('tbl_orders_3')\n",
    "spark.table(\"tbl_orders_3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38afb1a5-a5f4-4e14-b60e-fddbf554c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|order_mon| days_diffs|\n",
      "+---------+-----------+\n",
      "|  2025-09| [4, 6, 16]|\n",
      "|  2025-10|[5, 18, 24]|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"  \n",
    "  select \n",
    "    order_mon, \n",
    "    collect_list(days_diff) days_diffs\n",
    "   from tbl_orders_3\n",
    "   where days_diff > 0\n",
    "  group by order_mon \n",
    "  order by order_mon \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
