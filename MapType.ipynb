{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 10:00:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/02 10:00:45 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/03/02 10:00:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('MapType').setMaster('yarn') \n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed855099-5cfc-4de7-b477-841fcb5119bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import create_map, concat_ws, expr, col, lit, \\\n",
    "#    element_at, map_entries, map_from_arrays, array, map_keys, map_values, map_from_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef3e5a3-d39e-4458-9858-0319b590545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'map(k1, v1, k2, v2, k3, v3)'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "print(map_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2fb237-ebd4-4c5a-8f88-b9c73840f574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+-----------------------------+\n",
      "|k1 |v1 |k2 |v2 |k3 |v3 |map_data                     |\n",
      "+---+---+---+---+---+---+-----------------------------+\n",
      "|1  |aa |x  |0.5|1.0|_  |{1 -> aa, x -> 0.5, 1.0 -> _}|\n",
      "|2  |bb |y  |0.6|2.0|$  |{2 -> bb, y -> 0.6, 2.0 -> $}|\n",
      "|3  |cc |z  |0.7|3.0|^  |{3 -> cc, z -> 0.7, 3.0 -> ^}|\n",
      "+---+---+---+---+---+---+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df2 = df.withColumn('map_data', map_col)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33c34e8-9c37-4979-97e6-f8eceb878a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- k1: integer (nullable = true)\n",
      " |-- v1: string (nullable = true)\n",
      " |-- k2: string (nullable = true)\n",
      " |-- v2: float (nullable = true)\n",
      " |-- k3: float (nullable = true)\n",
      " |-- v3: string (nullable = true)\n",
      " |-- map_data: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.createDataFrame(\n",
    "#         [(1, 'aa', '1', 0.5, 1.0, '_'), \n",
    "#          (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "#          (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "#          \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df2 = df.withColumn('map_data', map_col)\n",
    "# df2.show(truncate=False)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6eb0d4-21f4-41cb-91d0-b9d4af2cb98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+------------------------------+\n",
      "|k1 |v1 |k2 |v2 |k3 |v3 |map_data                      |\n",
      "+---+---+---+---+---+---+------------------------------+\n",
      "|1  |aa |x  |0.5|1.0|_  |{1 ->  odd, literal_k2 -> 0.5}|\n",
      "|2  |bb |y  |0.6|2.0|$  |{2 -> even, literal_k2 -> 0.6}|\n",
      "|3  |cc |z  |0.7|3.0|^  |{3 ->  odd, literal_k2 -> 0.7}|\n",
      "+---+---+---+---+---+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, lit\n",
    "\n",
    "map_col = create_map(\n",
    "    'k1', expr(\"case when k1%2 = 0 then 'even' else ' odd' end\"),\n",
    "    lit('literal_k2'), 'v2',)\n",
    "\n",
    "df3 = df.withColumn('map_data', map_col)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e87b46-5dbf-46e1-be05-bfff80dea7b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+------------+-------------+\n",
      "|map_data                     |val_by_key_1|val_by_key_y|val_by_key_30|\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|aa          |null        |null         |\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|null        |0.6         |null         |\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|null        |null        |^            |\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df4 = df.withColumn('map_data', map_col)\n",
    "df4 = df4.withColumn('val_by_key_1', df4.map_data['1'])\\\n",
    "         .withColumn('val_by_key_y', df4['map_data']['y'])\\\n",
    "         .withColumn('val_by_key_30', col('map_data')['3.0'])\n",
    "\n",
    "df4.select('map_data', 'val_by_key_1', 'val_by_key_y', \\\n",
    "            'val_by_key_30').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8e3baf-f561-4e30-aaee-94387654f061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+------------+-------------+\n",
      "|map_data                     |val_by_key_1|val_by_key_y|val_by_key_30|\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|aa          |null        |null         |\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|null        |0.6         |null         |\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|null        |null        |^            |\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import element_at\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df4 = df.withColumn('map_data', map_col)\n",
    "df4 = df4.withColumn('val_by_key_1', element_at('map_data', '1'))\\\n",
    "         .withColumn('val_by_key_y', element_at('map_data', 'y'))\\\n",
    "         .withColumn('val_by_key_30', element_at('map_data', '3.0'))\n",
    "\n",
    "df4.select('map_data', 'val_by_key_1', 'val_by_key_y', \\\n",
    "            'val_by_key_30').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0d55db-8f62-453f-961e-aed417eb0bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+----------+-----------+\n",
      "|map_data                     |contains_1|contains_Y|contains_30|\n",
      "+-----------------------------+----------+----------+-----------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|true      |false     |false      |\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|false     |false     |false      |\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|false     |false     |true       |\n",
      "+-----------------------------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_contains_key\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df4 = df.withColumn('map_data', map_col)\n",
    "df4 = df4.withColumn('contains_1', map_contains_key('map_data', '1'))\\\n",
    "         .withColumn('contains_Y', map_contains_key('map_data', 'Y'))\\\n",
    "         .withColumn('contains_30', map_contains_key('map_data', '3.0'))\n",
    "\n",
    "df4.select('map_data', 'contains_1', 'contains_Y', \\\n",
    "            'contains_30').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5953e7f-c22d-4794-9998-073a62bc2ebb",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93d13aa-16fd-4e99-8a61-a7c18281480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------+------------+\n",
      "|map_data                     |keys       |values      |\n",
      "+-----------------------------+-----------+------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|[1, x, 1.0]|[aa, 0.5, _]|\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|[2, y, 2.0]|[bb, 0.6, $]|\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|[3, z, 3.0]|[cc, 0.7, ^]|\n",
      "+-----------------------------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, col, map_keys, map_values\n",
    "\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df2 = df.withColumn('map_data', map_col)\\\n",
    "        .withColumn('keys',   map_keys(col('map_data')))\\\n",
    "        .withColumn('values', map_values(col('map_data')))\n",
    "df2.select('map_data', 'keys', 'values').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bc7d9ab-a110-4533-8b59-90f90c1a1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+\n",
      "|map_data                     |array_map_entries            |\n",
      "+-----------------------------+-----------------------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|[{1, aa}, {x, 0.5}, {1.0, _}]|\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|[{2, bb}, {y, 0.6}, {2.0, $}]|\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|[{3, cc}, {z, 0.7}, {3.0, ^}]|\n",
      "+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_entries\n",
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, \\\n",
    "            k3: float, v3: string\")\n",
    "\n",
    "df5 = df.withColumn('map_data', map_col)\\\n",
    "        .withColumn('array_map_entries', map_entries('map_data'))\n",
    "df5 = df5.select('map_data', 'array_map_entries')\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02258a84-4904-426e-9b53-27cc5a1e6bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "|map_data                     |to_array_from_map            |back_to_map_from_array       |\n",
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|[{1, aa}, {x, 0.5}, {1.0, _}]|{1 -> aa, x -> 0.5, 1.0 -> _}|\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|[{2, bb}, {y, 0.6}, {2.0, $}]|{2 -> bb, y -> 0.6, 2.0 -> $}|\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|[{3, cc}, {z, 0.7}, {3.0, ^}]|{3 -> cc, z -> 0.7, 3.0 -> ^}|\n",
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_entries, map_from_entries\n",
    "\n",
    "df6 = df.withColumn('map_data', map_col)\\\n",
    "        .withColumn('to_array_from_map',      map_entries('map_data'))\\\n",
    "        .withColumn('back_to_map_from_array', map_from_entries('to_array_from_map'))\n",
    "df6 = df6.select('map_data', 'to_array_from_map', 'back_to_map_from_array')\n",
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f00fa132-64af-476e-aa77-e8912acb2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# generate keys list list with python\n",
    "key_list = ['k' + str(i) for i in range(1,6)]  \n",
    "print(type(key_list[0]))\n",
    "\n",
    "key_list = list(map(lambda x: lit(x), key_list))\n",
    "print(type(key_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9de6c31-6d11-4f87-b037-45d438429c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------+\n",
      "|id |map_data                                       |\n",
      "+---+-----------------------------------------------+\n",
      "|0  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "|1  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "|2  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "+---+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, array, map_from_arrays\n",
    "\n",
    "df = spark.range(3) # generates DataFrame with column 'id'\n",
    "\n",
    "# generate keys list list with python\n",
    "key_list = ['k' + str(i) for i in range(1,6)]  \n",
    "key_list = list(map(lambda x: lit(x), key_list))\n",
    "# generate values list list with python as power of 2\n",
    "val_list = [2**i for i in range(1,6)] \n",
    "val_list = list(map(lambda x: lit(x), val_list))\n",
    "\n",
    "map_col = map_from_arrays(array(key_list), array(val_list))\n",
    "\n",
    "df7 = df.withColumn('map_data', map_col)\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "304b480f-e2d9-4678-99b4-d1ba180b7e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|id |map_data                      |\n",
      "+---+------------------------------+\n",
      "|0  |{k1 -> v1}                    |\n",
      "|1  |{k1 -> v1, k2 -> v2}          |\n",
      "|2  |{k1 -> v1, k2 -> v2, k3 -> v3}|\n",
      "|3  |{k1 -> v1}                    |\n",
      "|4  |{k1 -> v1, k2 -> v2}          |\n",
      "+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, lit, when\n",
    "\n",
    "map_col1 = create_map(lit('k1'), lit('v1'))\n",
    "map_col2 = create_map(lit('k1'), lit('v1'), lit('k2'), lit('v2'))\n",
    "map_col3 = create_map(lit('k1'), lit('v1'), lit('k2'), lit('v2'), \\\n",
    "                      lit('k3'), lit('v3'))\n",
    "\n",
    "df = spark.range(5) #creates DataFrame with column 'id'  \n",
    "df8 = df.withColumn('map_data', when(df['id']%3 == 0, map_col1)\\\n",
    "                               .when(df['id']%3 == 1, map_col2)\\\n",
    "                               .when(df['id']%3 == 2, map_col3))\n",
    "df8.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfb177a6-3848-4df7-a2f3-2d8b538d2b81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column `id` has a data type of bigint, which is not supported by Text.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmap_to_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column `id` has a data type of bigint, which is not supported by Text."
     ]
    }
   ],
   "source": [
    "df8.coalesce(1).write.save(path='map_to_text', format='text', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "114a4a9a-10cd-4a6d-94d7-728dd62ede44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column `map_data` has a data type of map<string,string>, which is not supported by CSV.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmap_to_csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1398\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column `map_data` has a data type of map<string,string>, which is not supported by CSV."
     ]
    }
   ],
   "source": [
    "df8.coalesce(1).write.save(path='map_to_csv', format='csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c713b890-5aea-4114-988f-589662cf6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df8.coalesce(1).write.save(path='map_to_json', format='json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b73b1f-465e-4d41-827a-3395ae9e31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = spark.read.json(path='map_from_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
