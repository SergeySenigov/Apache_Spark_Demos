{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/26 18:08:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/26 18:08:06 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/02/26 18:08:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('MapType').setMaster('yarn') \n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed855099-5cfc-4de7-b477-841fcb5119bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import create_map, concat_ws, expr, col, lit, \\\n",
    "#    element_at, map_entries, map_from_arrays, array, map_keys, map_values, map_from_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef3e5a3-d39e-4458-9858-0319b590545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'map(k1, v1, k2, v2, k3, v3)'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "print(map_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2fb237-ebd4-4c5a-8f88-b9c73840f574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+-----------------------------+\n",
      "|k1 |v1 |k2 |v2 |k3 |v3 |map_data                     |\n",
      "+---+---+---+---+---+---+-----------------------------+\n",
      "|1  |aa |x  |0.5|1.0|_  |{1 -> aa, x -> 0.5, 1.0 -> _}|\n",
      "|2  |bb |y  |0.6|2.0|$  |{2 -> bb, y -> 0.6, 2.0 -> $}|\n",
      "|3  |cc |z  |0.7|3.0|^  |{3 -> cc, z -> 0.7, 3.0 -> ^}|\n",
      "+---+---+---+---+---+---+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df2 = df.withColumn('map_data', map_col)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33c34e8-9c37-4979-97e6-f8eceb878a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- k1: integer (nullable = true)\n",
      " |-- v1: string (nullable = true)\n",
      " |-- k2: string (nullable = true)\n",
      " |-- v2: float (nullable = true)\n",
      " |-- k3: float (nullable = true)\n",
      " |-- v3: string (nullable = true)\n",
      " |-- map_data: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.createDataFrame(\n",
    "#         [(1, 'aa', '1', 0.5, 1.0, '_'), \n",
    "#          (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "#          (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "#          \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df2 = df.withColumn('map_data', map_col)\n",
    "# df2.show(truncate=False)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6eb0d4-21f4-41cb-91d0-b9d4af2cb98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+------------------------------+\n",
      "|k1 |v1 |k2 |v2 |k3 |v3 |map_data                      |\n",
      "+---+---+---+---+---+---+------------------------------+\n",
      "|1  |aa |x  |0.5|1.0|_  |{1 ->  odd, literal_k2 -> 0.5}|\n",
      "|2  |bb |y  |0.6|2.0|$  |{2 -> even, literal_k2 -> 0.6}|\n",
      "|3  |cc |z  |0.7|3.0|^  |{3 ->  odd, literal_k2 -> 0.7}|\n",
      "+---+---+---+---+---+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, lit\n",
    "\n",
    "map_col = create_map(\n",
    "    'k1', expr(\"case when k1%2 = 0 then 'even' else ' odd' end\"),\n",
    "    lit('literal_k2'), 'v2',)\n",
    "\n",
    "df3 = df.withColumn('map_data', map_col)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8e3baf-f561-4e30-aaee-94387654f061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+------------+-------------+\n",
      "|map_data                     |val_by_key_1|val_by_key_y|val_by_key_30|\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|aa          |null        |null         |\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|null        |0.6         |null         |\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|null        |null        |^            |\n",
      "+-----------------------------+------------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import element_at\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df4 = df.withColumn('map_data', map_col)\n",
    "df4 = df4.withColumn('val_by_key_1', element_at('map_data', '1'))\\\n",
    "         .withColumn('val_by_key_y', element_at('map_data', 'y'))\\\n",
    "         .withColumn('val_by_key_30', element_at('map_data', '3.0'))\n",
    "\n",
    "df4.select('map_data', 'val_by_key_1', 'val_by_key_y', \\\n",
    "            'val_by_key_30').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0d55db-8f62-453f-961e-aed417eb0bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+----------+-----------+\n",
      "|map_data                     |contains_1|contains_Y|contains_30|\n",
      "+-----------------------------+----------+----------+-----------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|true      |false     |false      |\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|false     |false     |false      |\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|false     |false     |true       |\n",
      "+-----------------------------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_contains_key\n",
    "map_col = create_map('k1', 'v1', 'k2', 'v2', 'k3', 'v3')\n",
    "\n",
    "df4 = df.withColumn('map_data', map_col)\n",
    "df4 = df4.withColumn('contains_1', map_contains_key('map_data', '1'))\\\n",
    "         .withColumn('contains_Y', map_contains_key('map_data', 'Y'))\\\n",
    "         .withColumn('contains_30', map_contains_key('map_data', '3.0'))\n",
    "\n",
    "df4.select('map_data', 'contains_1', 'contains_Y', \\\n",
    "            'contains_30').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bc7d9ab-a110-4533-8b59-90f90c1a1f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+\n",
      "|map_data                     |array_map_entries            |\n",
      "+-----------------------------+-----------------------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|[{1, aa}, {x, 0.5}, {1.0, _}]|\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|[{2, bb}, {y, 0.6}, {2.0, $}]|\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|[{3, cc}, {z, 0.7}, {3.0, ^}]|\n",
      "+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_entries\n",
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, \\\n",
    "            k3: float, v3: string\")\n",
    "\n",
    "df5 = df.withColumn('map_data', map_col)\\\n",
    "        .withColumn('array_map_entries', map_entries('map_data'))\n",
    "df5 = df5.select('map_data', 'array_map_entries')\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02258a84-4904-426e-9b53-27cc5a1e6bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "|map_data                     |array_map_entries            |map_from_array               |\n",
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "|{1 -> aa, x -> 0.5, 1.0 -> _}|[{1, aa}, {x, 0.5}, {1.0, _}]|{1 -> aa, x -> 0.5, 1.0 -> _}|\n",
      "|{2 -> bb, y -> 0.6, 2.0 -> $}|[{2, bb}, {y, 0.6}, {2.0, $}]|{2 -> bb, y -> 0.6, 2.0 -> $}|\n",
      "|{3 -> cc, z -> 0.7, 3.0 -> ^}|[{3, cc}, {z, 0.7}, {3.0, ^}]|{3 -> cc, z -> 0.7, 3.0 -> ^}|\n",
      "+-----------------------------+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_from_entries\n",
    "df = spark.createDataFrame(\n",
    "        [(1, 'aa', 'x', 0.5, 1.0, '_'), \n",
    "         (2, 'bb', 'y', 0.6, 2.0, '$'), \n",
    "         (3, 'cc', 'z', 0.7, 3.0, '^')], \\\n",
    "         \"k1: int, v1: string, k2: string, v2: float, k3: float, v3: string\")\n",
    "\n",
    "df6 = df.withColumn('map_data', map_col)\\\n",
    "        .withColumn('array_map_entries', map_entries('map_data'))\\\n",
    "        .withColumn('map_from_array', map_from_entries('array_map_entries'))\n",
    "df6 = df6.select('map_data', 'array_map_entries', 'map_from_array')\n",
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "304b480f-e2d9-4678-99b4-d1ba180b7e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|id |map_data                      |\n",
      "+---+------------------------------+\n",
      "|0  |{k1 -> v1}                    |\n",
      "|1  |{k1 -> v1, k2 -> v2}          |\n",
      "|2  |{k1 -> v1, k2 -> v2, k3 -> v3}|\n",
      "|3  |{k1 -> v1}                    |\n",
      "|4  |{k1 -> v1, k2 -> v2}          |\n",
      "|5  |{k1 -> v1, k2 -> v2, k3 -> v3}|\n",
      "|6  |{k1 -> v1}                    |\n",
      "+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "map_col1 = create_map(lit('k1'), lit('v1'))\n",
    "map_col2 = create_map(lit('k1'), lit('v1'), lit('k2'), lit('v2'))\n",
    "map_col3 = create_map(lit('k1'), lit('v1'), lit('k2'), lit('v2'), \\\n",
    "                      lit('k3'), lit('v3'))\n",
    "\n",
    "df = spark.range(7) #creates DataFrame with column 'id'  \n",
    "df7 = df.withColumn('map_data', when(df['id']%3 == 0, map_col1)\\\n",
    "                               .when(df['id']%3 == 1, map_col2)\\\n",
    "                               .when(df['id']%3 == 2, map_col3))\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c713b890-5aea-4114-988f-589662cf6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.coalesce(1).write.save(path='map_to_json', format='json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9de6c31-6d11-4f87-b037-45d438429c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------+\n",
      "|id |map_data                                       |\n",
      "+---+-----------------------------------------------+\n",
      "|0  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "|1  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "|2  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|\n",
      "+---+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(3)\n",
    "\n",
    "key_list = ['k' + str(i) for i in range(1,6)] # generate keys list list with python \n",
    "key_list = list(map(lambda x: lit(x), key_list))\n",
    "\n",
    "val_list = [2**i for i in range(1,6)] # generate values list list with python as power of 2\n",
    "val_list = list(map(lambda x: lit(x), val_list))\n",
    "\n",
    "map_col = map_from_arrays(array(key_list), array(val_list))\n",
    "\n",
    "df8 = df.withColumn('map_data', map_col)\n",
    "df8.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0186b29-4b9e-4b5a-b9d0-24022f8375dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------+--------------------+-----------------+\n",
      "|id |map_data                                       |keys                |values           |\n",
      "+---+-----------------------------------------------+--------------------+-----------------+\n",
      "|0  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|[k1, k2, k3, k4, k5]|[2, 4, 8, 16, 32]|\n",
      "|1  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|[k1, k2, k3, k4, k5]|[2, 4, 8, 16, 32]|\n",
      "|2  |{k1 -> 2, k2 -> 4, k3 -> 8, k4 -> 16, k5 -> 32}|[k1, k2, k3, k4, k5]|[2, 4, 8, 16, 32]|\n",
      "+---+-----------------------------------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9 = df8.withColumn('keys', map_keys('map_data'))\\\n",
    "         .withColumn('values', map_values('map_data'))\n",
    "df9.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
