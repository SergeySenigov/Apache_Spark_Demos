{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf().setAppName('NoShuffleWithBroadcast').setMaster('yarn') \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false'))\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4951d792-aa84-4301-8507-07cf33bfc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_id                                   application_1727681258360_0044\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(\"app_id\".ljust(40), sc.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78ba495-3da2-4787-99f4-476e14fe7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions = 200\n",
      "df partitions num = 4\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame of length 1000 with 4 partitions with column 'val' \n",
    "partitions_num = 4\n",
    "data = [ [k] for k in range(0, 1000) ] # transform to 2-D list\n",
    "df = spark.sparkContext.parallelize(data, partitions_num).toDF([\"val\"])\n",
    "\n",
    "# view partitions num we get when doing shuffle \n",
    "print(\"spark.sql.shuffle.partitions =\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "\n",
    "print(\"df partitions num =\", df.rdd.getNumPartitions()) # initial partitions num equals 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dd7b2d-f2a0-49cc-9b97-c0763f99f2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_joined partitions num = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_joined_with_broacasted partitions num = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df2 = df.withColumnRenamed('val', 'val2') # to avoid column names ambiguity when joining\n",
    "\n",
    "# make common join \n",
    "df_joined = df.join(df2, df['val'] == df2['val2'], \"inner\")\n",
    "print(\"df_joined partitions num =\", df_joined.rdd.getNumPartitions()) \n",
    "# df_shuffled partitions num = 200 - there was shuffle\n",
    "\n",
    "df_broadcasted = broadcast(df2) # make broadcasted df and join with it\n",
    "df_joined_with_broacasted = df.join(df_broadcasted, df[\"val\"] == df_broadcasted[\"val2\"], \"inner\")\n",
    "# df_joined_with_broacasted.count()\n",
    "print(\"df_joined_with_broacasted partitions num =\", df_joined_with_broacasted.rdd.getNumPartitions()) \n",
    "# df_joined partitions num = 4 - there was no shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
