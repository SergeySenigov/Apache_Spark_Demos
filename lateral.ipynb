{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf().setAppName('lateral').setMaster('yarn') \n",
    "    .set('spark.sql.adaptive.enabled', False)  )\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print('app_id'.ljust(40), spark.sparkContext.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f145ea-6571-48b2-bae0-f89583efcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|emp_name|middle_name|\n",
      "+------+--------+-----------+\n",
      "|     1|    Anna|         E.|\n",
      "|     2|     Bob|       C.C.|\n",
      "|     3|   Clara|         R.|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "   select emp_id, emp_name, middle_name\n",
    "    from values (1, 'Anna', 'E.'), \n",
    "                (2, 'Bob', 'C.C.'), \n",
    "                (3, 'Clara', 'R.')\n",
    "      as (emp_id, emp_name, middle_name) \"\"\"\n",
    "\n",
    "df_emps = spark.sql(sql)\n",
    "spark.sql(\" select * from {df} \", df=df_emps).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748d54c8-bc5c-4dca-90fd-89647d318c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---+\n",
      "|emp_id|   pos_name|ord|\n",
      "+------+-----------+---+\n",
      "|     1| Jr Manager|  1|\n",
      "|     1|Mid Manager|  2|\n",
      "|     2|     Assist|  1|\n",
      "|     2|   Jr Sales|  2|\n",
      "|     2| Sales Mngr|  3|\n",
      "|     3|Office Mngr|  1|\n",
      "+------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "   select emp_id, pos_name, ord\n",
    "    from values\n",
    "       (1, 'Jr Manager',  1), \n",
    "       (1, 'Mid Manager', 2), -- Anna\n",
    "       \n",
    "       (2, 'Assist',      1), \n",
    "       (2, 'Jr Sales',    2), \n",
    "       (2, 'Sales Mngr',  3), -- Bob\n",
    "       \n",
    "       (3, 'Office Mngr', 1) -- Clara\n",
    "          as (emp_id, pos_name, ord) \"\"\" \n",
    "df_pos = spark.sql(sql)\n",
    "\n",
    "spark.sql(\" select * from {df} \", df=df_pos).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb86195-d197-418e-ab1b-5ff4a6c0d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|emp_name|full_name|\n",
      "+--------+---------+\n",
      "|Anna    |ANNA E.  |\n",
      "|Bob     |BOB C.C. |\n",
      "|Clara   |CLARA R. |\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without lateral join  \n",
    "sql = \"\"\"\n",
    "   select \n",
    "     emp_name, \n",
    "     concat_ws(' ', upper(emp_name), middle_name) full_name\n",
    "    from {emps} \n",
    "   order by emp_id\n",
    "  \"\"\"\n",
    "spark.sql(sql, emps=df_emps).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11796375-62dc-4cf8-abe5-0ba267654f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|emp_name|full_name|\n",
      "+--------+---------+\n",
      "|Anna    |ANNA E.  |\n",
      "|Bob     |BOB C.C. |\n",
      "|Clara   |CLARA R. |\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with lateral join  \n",
    "sql = \"\"\"\n",
    "   select emp_name, full_name\n",
    "    from {emps},\n",
    "      lateral \n",
    "         (select concat_ws(' ', upper(emp_name), middle_name) full_name ) \n",
    "   order by emp_id\n",
    "  \"\"\"\n",
    "spark.sql(sql, emps=df_emps).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc585c1-f6f8-4b27-b921-b4d186e4343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+------------------------------+--------------+\n",
      "|emp_id|emp_name|first_pos  |pos_list                      |current_pos   |\n",
      "+------+--------+-----------+------------------------------+--------------+\n",
      "|1     |Anna    |Jr Manager |[Jr Manager, Mid Manager]     |2. Mid Manager|\n",
      "|2     |Bob     |Assist     |[Assist, Jr Sales, Sales Mngr]|3. Sales Mngr |\n",
      "|3     |Clara   |Office Mngr|[Office Mngr]                 |1. Office Mngr|\n",
      "+------+--------+-----------+------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with lateral join  \n",
    "sql = \"\"\"\n",
    "   select emp_id, emp_name, first_pos, pos_list, current_pos\n",
    "    from {emps} e,\n",
    "     lateral (select p.ord, p.pos_name first_pos\n",
    "               from {pos} p \n",
    "              where p.emp_id = e.emp_id \n",
    "              and p.ord = 1),               -- first_position\n",
    "     lateral (select collect_list(pos_name) pos_list\n",
    "               from {pos} p \n",
    "              where p.emp_id = e.emp_id \n",
    "              group by p.emp_id),         -- all positions list\n",
    "     lateral (select max(ord) maxord\n",
    "               from {pos} p \n",
    "              where p.emp_id = e.emp_id), -- current position index\n",
    "     lateral (select concat_ws('. ', p.ord, p.pos_name) current_pos\n",
    "               from {pos} p \n",
    "              where p.emp_id = e.emp_id\n",
    "              and p.ord = maxord)      -- current position  \"\"\"\n",
    "\n",
    "spark.sql(sql, emps=df_emps, pos=df_pos).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
