{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce6b1dc-be57-4247-a04f-3344b4f0ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['PYSPARK_PYTHON'] =  'python3.9'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3.9'\n",
    "os.environ['HADOOP_USER_NAME']='ssenigov'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7c2edf-43ce-4a2e-ac39-be7a1964b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/30 15:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/30 15:51:32 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/12/30 15:51:33 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_id                                   application_1727681258360_0065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPySpark application consists of two parts: one is driver and the other is run in distributed containers on working nodes.\\nThey are separate processes and we can’t exchange data between them as we would do in a standard python program with variables and data objects.\\n\\nDriver doesn’t see the processed data until we ASK for it for example with DataFrame.show() or rdd.collect().\\n\\nAfter all, Apache Spark as well as MapReduce paradigm slogan is «Move code to data», not data to code.\\nAnd Apache Spark does it! At least with objects RDD and DataFrames – their data is distributed over workers in cluster and CODE of their methods as well distributed over workers in cluster.\\n\\nBut how one can have CUSTOM application CODE executed distributed over workers in cluster?\\n\\nImagine we’re building an antispam system. Distributed workers receive messages and set SPAM or NOT_SPAM tag. We’ve got a custom python function «get_tag_message(message)».\\n\\nHow can we run this custom python function distributed?\\n\\nThere are 3 simple ways, using functionality of RDD and DataFrame:\\nUse RDD.map(). With map() we can modify value in RDD.\\nUse DataFrame.foreach() or DataFrame.foreachPartition(). With foreach() we can NOT modify value in DataFrame, just make any custom operations with it\\nUse User-Defined Function within DataFrame With udf we can put its return value to a new column.\\n\\nLook at the examples in the code.\\n\\nAnd look! We are not obliged to use data from RDD or Dataframe in custom function. \\nThis can be just a distributed placeholder for data retrieving or generation and processing, like in a calculation of PI in one of previous posts.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = (SparkConf().setAppName('CodeToCluster').setMaster('yarn') \n",
    "    .set('spark.sql.adaptive.enabled', 'false'))\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"app_id\".ljust(40), spark.sparkContext.applicationId)\n",
    "\n",
    "'''\n",
    "PySpark application consists of two parts: one is driver and the other is run in distributed containers on working nodes.\n",
    "They are separate processes and we can’t exchange data between them as we would do in a standard python program with variables and data objects.\n",
    "\n",
    "Driver doesn’t see the processed data until we ASK for it for example with DataFrame.show() or rdd.collect().\n",
    "\n",
    "After all, Apache Spark as well as MapReduce paradigm slogan is «Move code to data», not data to code.\n",
    "And Apache Spark does it! At least with objects RDD and DataFrames – their data is distributed over workers in cluster and CODE of their methods as well distributed over workers in cluster.\n",
    "\n",
    "But how one can have CUSTOM application CODE executed distributed over workers in cluster?\n",
    "\n",
    "Imagine we’re building an antispam system. Distributed workers receive messages and set SPAM or NOT_SPAM tag. We’ve got a custom python function «get_tag_message(message)».\n",
    "\n",
    "How can we run this custom python function distributed?\n",
    "\n",
    "There are 3 simple ways, using functionality of RDD and DataFrame:\n",
    "Use RDD.map(). With map() we can modify value in RDD.\n",
    "Use DataFrame.foreach() or DataFrame.foreachPartition(). With foreach() we can NOT modify value in DataFrame, just make any custom operations with it\n",
    "Use User-Defined Function within DataFrame With udf we can put its return value to a new column.\n",
    "\n",
    "Look at the examples in the code.\n",
    "\n",
    "And look! We are not obliged to use data from RDD or Dataframe in custom function. \n",
    "This can be just a distributed placeholder for data retrieving or generation and processing, like in a calculation of PI in one of previous posts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7b601f-05ae-4dc5-b2a6-8e89fca66a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# return SPAM if 3 or more key words are found\n",
    "def get_tag_message(message: str): \n",
    "    WORDS_TO_FIND = ['discount', 'gain', 'only', 'today']\n",
    "    find_count = 0\n",
    "    \n",
    "    for word in WORDS_TO_FIND:\n",
    "        find_count += 1 if message.find(word) > -1 else 0\n",
    "\n",
    "        if find_count >= 3: \n",
    "            return 'SPAM'\n",
    "    return 'NOT_SPAM'\n",
    "\n",
    "# create a sample dataframe with 60 rows - take this list 10 times\n",
    "messages = ['only for you discount and gain', 'today for you discount',\n",
    "            'today discount 10$, profit', 'only today am i here',\n",
    "            'gain with discount today', 'today only you is at work']\n",
    "\n",
    "data = [ [k] for k in messages * 10 ] # take this 10 times\n",
    "df = spark.sparkContext.parallelize(data).toDF([\"msg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ef03ea-5be0-42d1-ae91-515c2c0197f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==================================================>    (185 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|tagged_msg                               |count|\n",
      "+-----------------------------------------+-----+\n",
      "|NOT_SPAM | only today am i here          |10   |\n",
      "|NOT_SPAM | today discount 10$, profit    |10   |\n",
      "|NOT_SPAM | today for you discount        |10   |\n",
      "|NOT_SPAM | today only you is at work     |10   |\n",
      "|SPAM     | gain with discount today      |10   |\n",
      "|SPAM     | only for you discount and gain|10   |\n",
      "+-----------------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use RDD.map(). With map() we can modify value in RDD\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def for_each_rdd_row(row):\n",
    "    message = row.msg # or row[0] or row['msg']\n",
    "    tag = get_tag_message(message) # call custom function\n",
    "    return Row(f\"{tag:8} | {message}\") # set tag as a part of value\n",
    "\n",
    "df_res = df.rdd.map(lambda r: for_each_rdd_row(r)).toDF([\"tagged_msg\"])\n",
    "df_res.groupBy('tagged_msg').count().orderBy(['tagged_msg']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81149cce-20d2-45c6-a0b4-bbe53cf5c69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: NOT_SPAM: 40, SPAM: 20\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame.foreach() \n",
    "# With foreach() we can NOT modify value in DataFrame, just make any custom operations with it\n",
    "from pyspark import Accumulator\n",
    "\n",
    "count_spam = spark.sparkContext.accumulator(0) # use accumulator to get result in driver\n",
    "count_not_spam = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def for_each_df_row(row):\n",
    "    global count_spam, count_not_spam \n",
    "    message = row.msg # or row[0] or row['msg']\n",
    "    tag = get_tag_message(message) # call custom function\n",
    "    if tag == 'SPAM':\n",
    "        count_spam +=1       # use accumulator to get result in the driver\n",
    "    elif tag == 'NOT_SPAM':\n",
    "        count_not_spam +=1   # use accumulator to get result in the driver \n",
    "\n",
    "df.foreach(for_each_df_row) # method doesn't return DF, just executes the function for each row\n",
    "print(f'Result: NOT_SPAM: {count_not_spam}, SPAM: {count_spam}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf091f9-dadb-4fce-a22e-bcfed49cb0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: NOT_SPAM: 40, SPAM: 20\n"
     ]
    }
   ],
   "source": [
    "# Use DataFrame.foreachPartition()\n",
    "# With foreachPartition() we can NOT modify values in DataFrame, just make any custom operations with it\n",
    "from pyspark import Accumulator\n",
    "\n",
    "count_spam = spark.sparkContext.accumulator(0) # use accumulator to get result in driver\n",
    "count_not_spam = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def for_each_df_partition(rows_iterator): \n",
    "    global count_spam, count_not_spam \n",
    "\n",
    "    for row in rows_iterator: # rows_iterator yields rows in processed data partition\n",
    "        message = row.msg # or row[0] or row['msg']\n",
    "        tag = get_tag_message(message) # call custom function\n",
    "        if tag == 'SPAM':\n",
    "            count_spam +=1       # use accumulator to get result in the driver\n",
    "        elif tag == 'NOT_SPAM':\n",
    "            count_not_spam +=1   # use accumulator to get result in the driver \n",
    "\n",
    "df.foreachPartition(for_each_df_partition) # method doesn't return DF, just executes the function for each row\n",
    "print(f'Result: NOT_SPAM: {count_not_spam}, SPAM: {count_spam}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774aac05-eceb-4cc3-9185-78ec6a9c6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>    (185 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|spam_tag|count|\n",
      "+--------+-----+\n",
      "|NOT_SPAM|40   |\n",
      "|SPAM    |20   |\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use User-Defined Function within DataFrame. With udf we can return its value to a new column.\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_get_tag_of_message = udf(lambda r: get_tag_message(r), StringType()) # define udf\n",
    "\n",
    "df.withColumn('spam_tag', udf_get_tag_of_message(col('msg'))) \\\n",
    "   .groupBy(['spam_tag']).count().orderBy(['spam_tag']).show(truncate=False) # call udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e244ed93-1a7d-452c-b8c2-b255d6656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
